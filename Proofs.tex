% LaTeX
\documentclass[10pt, letterpaper, oneside]{article}
\usepackage{amsfonts, amsmath, amssymb, amsthm, enumitem, graphicx, hanging, ifthen, lettrine, listings, microtype, multirow, natbib, pdfpages, rotating, scalefnt, setspace, textcomp, verbatim, xcolor, xspace}
\usepackage[utf8]{inputenc}\usepackage[T1]{fontenc}
% \usepackage{mathpazo}\usepackage{tgpagella}\usepackage[scaled]{luximono}
\usepackage[unicode=true,pdfusetitle,bookmarks=true,bookmarksnumbered=true,bookmarksopen=false,breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false]{hyperref}

\pagestyle{plain}%\onehalfspacing
%\usepackage[margin=1in]{geometry}

\title{Constraint Solver Thoughts}
\author{Kenny Foner}
\date{\today}

\newcommand{\I}{\text{I}}
\newcommand{\process}{\text{\bf process}}
\newcommand{\return}{\text{\bf return}}
\newcommand{\error}{\text{\bf error}}

\newcommand{\fv}{\mathrm{fv}}

\usepackage{thmtools}
\declaretheorem[numberwithin=section]{theorem}
\declaretheorem[sibling=theorem]{lemma}
\declaretheorem[sibling=theorem]{corollary}

\begin{document}
\maketitle

\section{Introduction}

This document is about figuring out how to show that GHC's internal constraint solver does what we all want it to do: the \emph{right thing}. In the process, we'll figure out what we mean by ``the right thing,'' formalize what it is that the existing solver actually does, and (hopefully) prove that these two things are equivalent. I'm writing in a deliberately half-informal style because that's what comes out of my head with least effort, and also, forcing myself to give intuition for what I'm saying is good for everybody involved.

\section{What is a Constraint Solver?}

When in the course of computational events it becomes necessary for one typechecker to figure out whether or not all the types agree in a program, we can use a constraint solver to determine an answer. Constraint-directed typechecking approaches separate typechecking into two phases: constraint \emph{generation} and constraint \emph{solving}.

First, we \emph{generate} a set of constraints by processing the program and its annotations. There are two kinds of constraints we discover in this process: \textsc{given} constraints---things we know are true---and \textsc{wanted} constraints---things we need to be true for a program to typecheck. A piece of code \(f\) that is only type-correct under a certain constraint \(c\) has \(c\) as a \textsc{given} constraint when we typecheck its insides. A piece of code calling \(f\) will have \(c\) as a \textsc{wanted} constraint.

What kinds of ``things'' can constraints talk about? In GHC, a lot of things: type equalities, but also typeclass instances, type family equations, functional dependencies, and more. To simplify the initial discussion, we'll focus for right now on just a treatment of type equalities. That is to say, every constraint we consider here is of the form \(\tau \sim \sigma\), where \(\tau\) and \(\sigma\) are types and \(\sim\) is nominal type equality. (We will extend this treatment to representational equality later, but not now.)

Second, we \emph{solve} the constraint sets we generated. A successful solution to a constraint solving problem means that we can deduce that the conjunction of all the given constraints implies each wanted constraint. Algorithms like the one in GHC do this in an iterative, stateful way. When we find out that a \textsc{wanted} constraint is satisfied, we remove it from consideration. (We elide here a discussion of producing evidence for equalities, which is another important function of the constraint solver but is not essential to a discussion of equality constraints.) A successful run of the solver means that we terminate in a state where we no longer have any \textsc{wanted} constraints left to solve. An unsuccessful run of the solver means that we terminate, having discovered that one or more {\textsc{wanted}}s are unsatisfiable.\\

\textbf{Aside:} Additionally, we would like to put in a good-faith effort to detect contradictions in the set of \textsc{given} constraints, because this gives the user earlier warning that they have written nonsense---but we will never run unsound code if we fail to report such contradictions, since code with unsatisfiable preconditions can never be run.\\

I claim that a constraint solver does the right thing if, for all inputs, it
\begin{itemize}
\item rejects if any \textsc{wanted} constraints are unsatisfiable \textbf{[soundness]}
\item accepts if all \textsc{wanted} constraints are satisfiable \textbf{[completeness]}
\item always finishes processing in bounded time \textbf{[termination]}
\end{itemize}

We shall try to prove that these properties hold for (some model of) GHC's constraint solver.

\section{Substitutions}

Constraint solvers like GHC's process an input set of constraints by iteratively constructing a \emph{substitution} over types. For reasons that will become apparent later, this is called the \emph{inert set}.

A \emph{substitution} is, abstractly, a total function from \emph{type variables} to \emph{types}. We can lift a substitution to be a function from \emph{types} to \emph{types} by uniformly applying it to every type variable in a given type. We will abuse notation in this way, by writing for a substitution \(S\) both \(S(\alpha)\) and \(S(\tau)\).

Substitutions can have various properties which are of interest to us in this discussion. To distinguish between equality of \emph{representations} of substitutions and \emph{extensional} equality of substitutions-as-functions, we write \(=\) for the former and \(\approx\) for the latter. That is, \(S \approx T \triangleq \forall \alpha, S(\alpha) = T(\alpha)\). (Here, equality on types is exact syntactic equality, with no notion of \(\alpha\)-equivalence.)

\begin{description}
\item[Finitude:] \(S\) is finite if \(\{\alpha \mid S(\alpha) \ne \alpha\}\) is finite (that is, \(S\) is the identity on all but finitely many type \emph{variables}---but may still be non-identity on infinitely many \emph{types})
\item[Acyclicity:] \(S\) is acyclic if \(\forall \alpha, S(\alpha) \ne \alpha \to \forall n > 0, \alpha \notin \fv(S^n(\alpha))\)
\item[Weak fixed point:] \(S\) has a weak fixed point if \(\forall \alpha, \exists n, S^n(\alpha) = S^{n + 1}(\alpha)\)
\item[Strong fixed point:] \(S\) has a strong fixed point if \(\exists n, \forall \alpha, S^n(\alpha) = S^{n + 1}(\alpha)\) (an equivalent statement to \(\exists n, S^n \approx S^{n + 1}\))
\item[Idempotence:] \(S\) is idempotent if \(S \approx S \circ S\)
\end{description}

\begin{lemma}
  \label{lemma:strong-weak}
  Strong and weak fixed points are equivalent for finite substitutions.
\end{lemma}

\begin{proof}
  We first show that if a substitution \(S\) has a strong fixed point, it has weak fixed points. This follows immediately from the observation \(\exists x, \forall y, P(x,y) \to \forall y, \exists x, P(x,y)\)---that is, it is trivial to push existential quantification under universal quantification (but not vice versa, of course).

  Now we must show that a weak fixed point implies the existence of a strong fixed point if the substitution is finite. We show this by taking a maximum over the number of iterations required to reach a fixed point for any given type variable. First, observe that for all reflexively-mapped type variables in the domain of \(S\), we need apply \(S\) zero times to reach a fixed point. We therefore consider only those variables not mapped to themselves in \(S\). Because \(S\) is finite, the set \(\{\alpha \mid S(\alpha) \ne \alpha\}\) is finite. For each \(\alpha\) in this set, there exists some \(n\) for which \(S^n(\alpha) = S^{n + 1}(\alpha)\), and by the well-ordering principle, there exists some unique smallest \(n\) for which this property holds. The maximum of all these numbers must also be a fixed point for any particular \(\alpha\), as the maximum is definitionally greater than or equal to the number of iterations needed to reach a fixed point for that type variable.
\end{proof}

\begin{lemma}
  \label{lemma:finite-acyclic-strong}
  A finite substitution is acyclic iff it has a strong fixed point.
\end{lemma}

\begin{proof}
  In the forward direction, we consider the set of type variables \(V\) on which an acyclic finite substitution \(S\) ``acts,'' \(V = \{\alpha \mid S(\alpha) \ne \alpha\}\). We show by induction that for a given \(\alpha \in V\), the greatest \(n\) for which \(S^n(\alpha) \ne S^{n + 1}(\alpha)\) is at most \(|V|\), which immediately gives us the strong fixed point property.

  By acyclicity, we know that for any \(\alpha \in V\), \(\alpha\) will not be present in the free variables of \(S^n(\alpha)\), for any \(n\) we choose. Thus, \(S^{n + 1}(\alpha)\) must be equivalent to \((S[\alpha \mapsto \alpha])^n(S(\alpha))\), since acyclicity tells us that \(\alpha\) will not be encountered again after we substitute for it.\footnote{A note on notation: we write \(S[\alpha \mapsto \tau]\) to denote the function equal to \(S(\beta)\) when \(\beta \ne \alpha\), and equal to \(\tau\) when \(\beta = \alpha\).} For all \(\alpha \in V\), \(S[\alpha \mapsto \alpha]\) acts on exactly one fewer variable: \(\alpha\). Thus, every application of \(S\) leaves one less potential variable to be rewritten, and since \(V\) is finite, we will run out of variables to rewrite in at most \(|V|\) steps.

  In the reverse direction, we consider a finite substitution that contains a cycle on a type variable \(\alpha\). This means that \(S(\alpha) \ne \alpha\) and for some \(m > 0\), \(S^m(\alpha)\) mentions \(\alpha\). We therefore know that \(S^{nm}(\alpha)\) must mention \(\alpha\), because of this \(m\)-length cycle on \(\alpha\). And since we further assumed that \(S(\alpha) \ne \alpha\), we know \(S^{nm}(\alpha) \ne S^{nm + 1}(\alpha)\). But since \(S\) supposedly has a strong fixed point of size \(n\), it must be that \(S^{nm}(\alpha) = S^{nm + 1}(\alpha)\), a contradiction.
\end{proof}

\begin{corollary}
  \label{corollary:idempotize}
  For every finite acyclic substitution \(S\), we can construct an idempotent substitution \(S^\ast\) equivalent to the least fixed point of \(S\), where \(S^\ast = S^{|\{\alpha \mid S(\alpha) \ne \alpha\}|}\).
\end{corollary}

\begin{proof}
  By Lemma~\ref{lemma:finite-acyclic-strong}, a finite acyclic substitution \(S\) has a strong fixed point---that is, there is some \(n\) such that \(S^n \approx S^{n + 1}\). This fixed point is definitionally idempotent. Moreover, the argument in Lemma~\ref{lemma:finite-acyclic-strong} tells us that \(n \le |V|\) where \(V = \{\alpha \mid S(\alpha) \ne \alpha\}\). As a result, we can construct an idempotent substitution \(S^\ast \triangleq S^{|V|}\) which is extensionally equal to the least fixed point of \(S\).
\end{proof}

\section{Representing Substitutions}

In a constraint solving algorithm, it is useful to represent substitutions not as abstract functions, but as concrete finite sets of ``atomic'' equational rewrites.

We define an \emph{atomic rewrite} to be a pair \((\alpha, \tau)\) of a type variable and a type. A \emph{concrete substitution} is a set \(C\) of atomic rewrites which is function-like, in that \(\forall\, (\alpha, \tau),\,(\beta, \sigma) \in C, \alpha = \beta \to \tau = \sigma\). We may treat \(C\) as a total function on type variables as follows:

\begin{equation*}
  C(\alpha) =
  \begin{cases}
    \tau & \exists!\, \tau, (\alpha, \tau) \in C\\
    \alpha & \textrm{otherwise}
  \end{cases}
\end{equation*}

Like our abstract treatment of substitutions, we may also lift this total function on type variables to one on types.

We will use properties like ``acyclic,'' ``idempotent,'' etc. freely when discussing concrete substitutions. These statements should be interpreted to refer to the functional interpretation of the substitution.

\begin{corollary}
  If \(C\) is acyclic, then \(C^{|C|}\) is idempotent.
\end{corollary}

\begin{proof}
  By the above definition of the functional interpretation of \(C\), the number of type variables on which \(C\) is not the identity is at most \(|C|\). By Corollary~\ref{corollary:idempotize}, the substitution \(C^{|C|}\) is thus idempotent.
\end{proof}

\section{Augmenting Substitutions}

One way of making a constraint solver is to build up a canonical concrete substitution (the \emph{inert set}) as we process input constraints. Arbitrary constraints are first canonicalized, which in the case of equality constraints transforms them into \emph{atomic rewrites}. We proceed structurally over types, using generativity/injectivity to split apart type constructor applications. For instance: the initial constraint \(\alpha\, \beta \sim \delta\, \gamma\) would be decomposed into \([\alpha \sim \delta,\, \beta \sim \gamma]\). At this point, we discover some impossible constraints, as we can rule out anything that equates a type constructor application with a constant.

When we encounter a new canonical \textsc{given} constraint, we wish to add it to the inert set. However, we must first ensure that doing so preserves key properties of the inert set: it must continue to be a concrete substitution (that is, functional) with a strong fixed point. We know by Lemma~\ref{lemma:finite-acyclic-strong} that this property is equivalent to acyclicity for finite substitutions. We present a sufficient set of criteria which together protect the inert set from problematic augmentation.

% When inserting the GIVEN (a |-> t):
% T1, T2: apply I.S. to work item's sides [always succeeds]
% T3: occurs check on work item [fail immediately]
% K1: for all s, no (a |-> s) in I.S. [kick it out] -- preserve functionality
% K2: trivial
% K3: for all b, no (b |-> a) in I.S. [kick it out] -- we don't need this??

% So we have that:
% a is not ``acted on'' by existing I.S. (I.S. maps a |-> a) [T1]
% no tyvar in t is ``acted on'' by existing I.S. [T2]
% a \notin fv(t) [T3]
% the resultant I.S. is still functional [K1]
% do we really need K3 at all?

% All potential long cycles get squished by T2 into loops in the input rewrite

% S has n-fixed-point ==>
% S^n . Id[a |-> t] . S^n = (S[a |-> t])^(2n + 1)

% Non-reflexive can-rewrite: an equality that can't be composed w/ itself????

For a given ``work item'' \((\alpha \sim \tau)\), we may insert it into the inert set \(S\) if:

\begin{description}
\item[(T1)] \(S^\ast(\alpha) = \alpha\)
\item[(T2)] \(S^\ast(\tau) = \tau\)
\item[(T3)] \(\alpha \notin \fv(\tau)\)
\end{description}

We show that if these conditions are met, an inert set \(S[\alpha \mapsto \tau]\) is still acyclic, and thus has a strong fixed point.

\begin{proof}
  Suppose not, that while \(S\) is acyclic and T1, T2 hold of \((\alpha \sim \tau)\), there is a cycle in \(S[\alpha \mapsto \tau]\). Any such cycle must involve the atomic substitution \(\alpha \mapsto \tau\), as otherwise it would exist in \(S\). This means that there must be a cycle on \(\alpha\). By T3, we know that such a cycle requires some free variable in \(\tau\) to be mapped by \(S^\ast\) to a type with \(\alpha\) as a free variable (this relies on functionality of \(S\)?). Yet by T2, we know that \(S^\ast\) is the identity on \(\tau\), which means that it cannot map any variable in \(\tau\) to \(\alpha\).
\end{proof}

% a -> b, c -> a, b -> c
% [a -> b, c -> b]

% Look at Conor's paper on unification again

\section{A Constraint Solving Algorithm}

We now present a simple algorithm which takes a set of \textsc{given} constraints and a set of \textsc{wanted} constraints, and determines if every \textsc{wanted} constraint is implied by the set of \textsc{given} constraints. We do this in two phases: first, we process the \textsc{given} constraints to form a composite concrete substitution called the ``inert set.'' During this process, we abort the algorithm if we detect obvious inconsistencies in the set of \textsc{given} constraints. If we produce a substitution, we apply it to each \textsc{wanted} in turn, and iff we can rewrite all of them to a reflexive equality (i.e. \(\tau \sim \tau\)), we succeed.

As noted in earlier sections, we would like to show \emph{soundness}, \emph{termination}, and \emph{completeness} of this algorithm.

To write specifically about such an algorithm, we must be precise about what grammar describes our types. Our types take the form
\begin{equation*}
  \tau ::= \text{\sc t} \mid \alpha \mid \tau\ \tau
\end{equation*}
where we write \(\tau, \sigma, \rho\, \pi\) for types, \(\alpha, \beta, \gamma\) for type variables, and {\sc t}, {\sc s} for ground types (constants) like Int, List, etc. We will extend this grammar later to include type-level functions.

The initial sets of \textsc{given} and \textsc{wanted} constraints are equalities between arbitrary types; that is, they are of the form \(\tau \sim \sigma\). We will call the set of yet-to-be-processed \textsc{given} equalities the ``work set'' (deviating from the nomenclature in GHC to emphasize that we will assume nothing about ordering within this structure). Initially, the work set is the set of \textsc{given} equalities. For each step of the solver, we remove an arbitrary \textsc{given} from the work set and process it. This processing may modify the inert set, and may also produce more \textsc{given} equalities to be deferred for later processing.
\begin{equation*}
  \text{\bf process} : \text{equality} \times \text{inert set} \to \text{error} + \text{set of equalities} \times \text{inert set}
\end{equation*}
We note that the type \(X + (Y \times \alpha)\) forms a monad (i.e. MaybeT \(X\) (Writer \(Y\)) \(\alpha\)), and use this to simplify presentation of this function.

\begin{alignat*}{2}
  &\text{\bf process}(\text{\sc t} \sim \text{\sc t}, \I) = \return(\,\I\,) &&\\
  &\text{\bf process}(\text{\sc t} \sim \text{\sc s}, \_) = \error(\text{``ground types not equal''}) &&\\
  &\text{\bf process}(\text{\sc t} \sim \sigma\, \rho, \_) = \error(\text{``not an application''}) &&\\
  &\text{\bf process}(\tau\, \pi \sim \sigma\, \rho, \I) = \text{\bf do} &&\\
  &~~~~I^\prime \leftarrow \text{\bf process}(\tau \sim\ \sigma, \I) &&\\
  &~~~~\text{\bf tell}(\{\pi \sim \rho\}) &&\\
  &~~~~\return(\,I^\prime\,) &&\\
  &\text{\bf process}(\alpha \sim \tau, \I) = \text{\bf do} &&\\
  &~~~~\text{\bf tell}(\{\alpha \sim \I(\alpha)\}) &&\\
  &~~~~\return\big(\,\I\,[\I^\ast(\alpha) \mapsto \I^\ast(\tau)]\,\big) &&
\end{alignat*}

\end{document}