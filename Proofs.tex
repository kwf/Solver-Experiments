% LaTeX
\documentclass[10pt, letterpaper, oneside]{article}
\usepackage{amsfonts, amsmath, amssymb, amsthm, enumitem, graphicx, hanging, ifthen, lettrine, listings, microtype, multirow, mathtools, natbib, pdfpages, rotating, scalefnt, setspace, textcomp, verbatim, xcolor, xspace}
\usepackage[utf8]{inputenc}\usepackage[T1]{fontenc}
% \usepackage{mathpazo}\usepackage{tgpagella}\usepackage[scaled]{luximono}
\usepackage[unicode=true,pdfusetitle,bookmarks=true,bookmarksnumbered=true,bookmarksopen=false,breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false]{hyperref}

\pagestyle{plain}%\onehalfspacing
%\usepackage[margin=1in]{geometry}

\title{Constraint Solver Thoughts}
\author{Kenny Foner}
\date{\today}

\newcommand{\inertset}{\text{I}}
\newcommand{\process}{\text{\bf process}}
\newcommand{\return}{\text{\bf return}}
\newcommand{\error}{\text{\bf error}}

\newcommand{\fv}{\mathrm{fv}}

\usepackage{thmtools}
\declaretheorem[numberwithin=section]{theorem}
\declaretheorem[sibling=theorem]{lemma}
\declaretheorem[sibling=theorem]{corollary}

\newenvironment{bcases}
  {\left\lbrace\begin{aligned}}
  {\end{aligned}\right\rbrace}

\begin{document}
\maketitle

\section{Introduction}

This document is about figuring out how to show that GHC's internal constraint solver does what we all want it to do: the \emph{right thing}. In the process, we'll figure out what we mean by ``the right thing,'' formalize what it is that the existing solver actually does, and (hopefully) prove that these two things are equivalent. I'm writing in a deliberately half-informal style because that's what comes out of my head with least effort, and also, forcing myself to give intuition for what I'm saying is good for everybody involved.

\section{What is a Constraint Solver?}

When in the course of computational events it becomes necessary for one typechecker to figure out whether or not all the types agree in a program, we can use a constraint solver to determine an answer. Constraint-directed typechecking approaches separate typechecking into two phases: constraint \emph{generation} and constraint \emph{solving}.

First, we \emph{generate} a set of constraints by processing the program and its annotations. There are two kinds of constraints we discover in this process: \textsc{given} constraints---things we know are true---and \textsc{wanted} constraints---things we need to be true for a program to typecheck. A piece of code \(f\) that is only type-correct under a certain constraint \(c\) has \(c\) as a \textsc{given} constraint when we typecheck its insides. A piece of code calling \(f\) will have \(c\) as a \textsc{wanted} constraint.

What kinds of ``things'' can constraints talk about? In GHC, a lot of things: type equalities, but also typeclass instances, type family equations, functional dependencies, and more. To simplify the initial discussion, we'll focus for right now on just a treatment of type equalities. That is to say, every constraint we consider here is of the form \(\tau \sim \sigma\), where \(\tau\) and \(\sigma\) are types and \(\sim\) is nominal type equality. (We will extend this treatment to representational equality later, but not now.)

Second, we \emph{solve} the constraint sets we generated. A successful solution to a constraint solving problem means that we can deduce that the conjunction of all the given constraints implies each wanted constraint. Algorithms like the one in GHC do this in an iterative, stateful way. When we find out that a \textsc{wanted} constraint is satisfied, we remove it from consideration. (We elide here a discussion of producing evidence for equalities, which is another important function of the constraint solver but is not essential to a discussion of equality constraints.) A successful run of the solver means that we terminate in a state where we no longer have any \textsc{wanted} constraints left to solve. An unsuccessful run of the solver means that we terminate, having discovered that one or more {\textsc{wanted}}s are unsatisfiable.\\

\textbf{Aside:} Additionally, we would like to put in a good-faith effort to detect contradictions in the set of \textsc{given} constraints, because this gives the user earlier warning that they have written nonsense---but we will never run unsound code if we fail to report such contradictions, since code with unsatisfiable preconditions can never be run.\\

I claim that a constraint solver does the right thing if, for all inputs, it
\begin{itemize}
\item rejects if any \textsc{wanted} constraints are unsatisfiable \textbf{[soundness]}
\item accepts if all \textsc{wanted} constraints are satisfiable \textbf{[completeness]}
\item always finishes processing in bounded time \textbf{[termination]}
\end{itemize}

We shall try to prove that these properties hold for (some model of) GHC's constraint solver.

\section{Substitutions}

Constraint solvers like GHC's process an input set of constraints by iteratively constructing a \emph{substitution} over types. For reasons that will become apparent later, this is called the \emph{inert set}.

A \emph{substitution} is, abstractly, a total function from \emph{type variables} to \emph{types}. We can lift a substitution to be a function from \emph{types} to \emph{types} by uniformly applying it to every type variable in a given type. We will abuse notation in this way, by writing for a substitution \(S\) both \(S(\alpha)\) and \(S(\tau)\).

Substitutions can have various properties which are of interest to us in this discussion. To distinguish between equality of \emph{representations} of substitutions and \emph{extensional} equality of substitutions-as-functions, we write \(=\) for the former and \(\approx\) for the latter. That is, \(S \approx T \triangleq \forall \alpha, S(\alpha) = T(\alpha)\). (Here, equality on types is exact syntactic equality, with no notion of \(\alpha\)-equivalence.)

\begin{description}
\item[Finitude:] \(S\) is finite if \(\{\alpha \mid S(\alpha) \ne \alpha\}\) is finite (that is, \(S\) is the identity on all but finitely many type \emph{variables}---but may still be non-identity on infinitely many \emph{types})
\item[Acyclicity:] \(S\) is acyclic if \(\forall \alpha, S(\alpha) \ne \alpha \to \forall n > 0, \alpha \notin \fv(S^n(\alpha))\)
\item[Weak fixed point:] \(S\) has a weak fixed point if \(\forall \alpha, \exists n, S^n(\alpha) = S^{n + 1}(\alpha)\)
\item[Strong fixed point:] \(S\) has a strong fixed point if \(\exists n, \forall \alpha, S^n(\alpha) = S^{n + 1}(\alpha)\) (an equivalent statement to \(\exists n, S^n \approx S^{n + 1}\))
\item[Idempotence:] \(S\) is idempotent if \(S \approx S \circ S\)
\end{description}

\begin{lemma}
  \label{lemma:strong-weak}
  Strong and weak fixed points are equivalent for finite substitutions.
\end{lemma}

\begin{proof}
  We first show that if a substitution \(S\) has a strong fixed point, it has weak fixed points. This follows immediately from the observation \(\exists x, \forall y, P(x,y) \to \forall y, \exists x, P(x,y)\)---that is, it is trivial to push existential quantification under universal quantification (but not vice versa, of course).

  Now we must show that a weak fixed point implies the existence of a strong fixed point if the substitution is finite. We show this by taking a maximum over the number of iterations required to reach a fixed point for any given type variable. First, observe that for all reflexively-mapped type variables in the domain of \(S\), we need apply \(S\) zero times to reach a fixed point. We therefore consider only those variables not mapped to themselves in \(S\). Because \(S\) is finite, the set \(\{\alpha \mid S(\alpha) \ne \alpha\}\) is finite. For each \(\alpha\) in this set, there exists some \(n\) for which \(S^n(\alpha) = S^{n + 1}(\alpha)\), and by the well-ordering principle, there exists some unique smallest \(n\) for which this property holds. The maximum of all these numbers must also be a fixed point for any particular \(\alpha\), as the maximum is definitionally greater than or equal to the number of iterations needed to reach a fixed point for that type variable.
\end{proof}

\begin{lemma}
  \label{lemma:finite-acyclic-strong}
  A finite substitution is acyclic iff it has a strong fixed point.
\end{lemma}

\begin{proof}
  In the forward direction, we consider the set of type variables \(V\) on which an acyclic finite substitution \(S\) ``acts,'' \(V = \{\alpha \mid S(\alpha) \ne \alpha\}\). We show by induction that for a given \(\alpha \in V\), the greatest \(n\) for which \(S^n(\alpha) \ne S^{n + 1}(\alpha)\) is at most \(|V|\), which immediately gives us the strong fixed point property.

  By acyclicity, we know that for any \(\alpha \in V\), \(\alpha\) will not be present in the free variables of \(S^n(\alpha)\), for any \(n\) we choose. Thus, \(S^{n + 1}(\alpha)\) must be equivalent to \((S[\alpha \mapsto \alpha])^n(S(\alpha))\), since acyclicity tells us that \(\alpha\) will not be encountered again after we substitute for it.\footnote{A note on notation: we write \(S[\alpha \mapsto \tau]\) to denote the function equal to \(S(\beta)\) when \(\beta \ne \alpha\), and equal to \(\tau\) when \(\beta = \alpha\).} For all \(\alpha \in V\), \(S[\alpha \mapsto \alpha]\) acts on exactly one fewer variable: \(\alpha\). Thus, every application of \(S\) leaves one less potential variable to be rewritten, and since \(V\) is finite, we will run out of variables to rewrite in at most \(|V|\) steps.

  In the reverse direction, we consider a finite substitution that contains a cycle on a type variable \(\alpha\). This means that \(S(\alpha) \ne \alpha\) and for some \(m > 0\), \(S^m(\alpha)\) mentions \(\alpha\). We therefore know that \(S^{nm}(\alpha)\) must mention \(\alpha\), because of this \(m\)-length cycle on \(\alpha\). And since we further assumed that \(S(\alpha) \ne \alpha\), we know \(S^{nm}(\alpha) \ne S^{nm + 1}(\alpha)\). But since \(S\) supposedly has a strong fixed point of size \(n\), it must be that \(S^{nm}(\alpha) = S^{nm + 1}(\alpha)\), a contradiction.
\end{proof}

\begin{corollary}
  \label{corollary:idempotize}
  For every finite acyclic substitution \(S\), we can construct an idempotent substitution \(S^\ast\) equivalent to the least fixed point of \(S\), where \(S^\ast = S^{|\{\alpha \mid S(\alpha) \ne \alpha\}|}\).
\end{corollary}

\begin{proof}
  By Lemma~\ref{lemma:finite-acyclic-strong}, a finite acyclic substitution \(S\) has a strong fixed point---that is, there is some \(n\) such that \(S^n \approx S^{n + 1}\). This fixed point is definitionally idempotent. Moreover, the argument in Lemma~\ref{lemma:finite-acyclic-strong} tells us that \(n \le |V|\) where \(V = \{\alpha \mid S(\alpha) \ne \alpha\}\). As a result, we can construct an idempotent substitution \(S^\ast \triangleq S^{|V|}\) which is extensionally equal to the least fixed point of \(S\).
\end{proof}

\section{Representing Substitutions}

In a constraint solving algorithm, it is useful to represent substitutions not as abstract functions, but as concrete finite sets of ``atomic'' equational rewrites.

We define an \emph{atomic rewrite} to be a pair \((\alpha, \tau)\) of a type variable and a type. A \emph{concrete substitution} is a set \(C\) of atomic rewrites which is function-like, in that \(\forall\, (\alpha, \tau),\,(\beta, \sigma) \in C, \alpha = \beta \to \tau = \sigma\). We may treat \(C\) as a total function on type variables as follows:

\begin{equation*}
  C(\alpha) =
  \begin{cases}
    \tau & \exists!\, \tau, (\alpha, \tau) \in C\\
    \alpha & \textrm{otherwise}
  \end{cases}
\end{equation*}

Like our abstract treatment of substitutions, we may also lift this total function on type variables to one on types.

We will use properties like ``acyclic,'' ``idempotent,'' etc. freely when discussing concrete substitutions. These statements should be interpreted to refer to the functional interpretation of the substitution.

\begin{corollary}
  If \(C\) is acyclic, then \(C^{|C|}\) is idempotent.
\end{corollary}

\begin{proof}
  By the above definition of the functional interpretation of \(C\), the number of type variables on which \(C\) is not the identity is at most \(|C|\). By Corollary~\ref{corollary:idempotize}, the substitution \(C^{|C|}\) is thus idempotent.
\end{proof}

\section{Augmenting Substitutions}

One way of making a constraint solver is to build up a canonical concrete substitution (the \emph{inert set}) as we process input constraints. Arbitrary constraints are first canonicalized, which in the case of equality constraints transforms them into \emph{atomic rewrites}. We proceed structurally over types, using generativity/injectivity to split apart type constructor applications. For instance: the initial constraint \(\alpha\, \beta \sim \delta\, \gamma\) would be decomposed into \([\alpha \sim \delta,\, \beta \sim \gamma]\). At this point, we discover some impossible constraints, as we can rule out anything that equates a type constructor application with a constant.

When we encounter a new canonical \textsc{given} constraint, we wish to add it to the inert set. However, we must first ensure that doing so preserves key properties of the inert set: it must continue to be a concrete substitution (that is, functional) with a strong fixed point. We know by Lemma~\ref{lemma:finite-acyclic-strong} that this property is equivalent to acyclicity for finite substitutions. We present a sufficient set of criteria which together protect the inert set from problematic augmentation.

% When inserting the GIVEN (a |-> t):
% T1, T2: apply I.S. to work item's sides [always succeeds]
% T3: occurs check on work item [fail immediately]
% K1: for all s, no (a |-> s) in I.S. [kick it out] -- preserve functionality
% K2: trivial
% K3: for all b, no (b |-> a) in I.S. [kick it out] -- we don't need this??

% So we have that:
% a is not ``acted on'' by existing I.S. (I.S. maps a |-> a) [T1]
% no tyvar in t is ``acted on'' by existing I.S. [T2]
% a \notin fv(t) [T3]
% the resultant I.S. is still functional [K1]
% do we really need K3 at all?

% All potential long cycles get squished by T2 into loops in the input rewrite

% S has n-fixed-point ==>
% S^n . Id[a |-> t] . S^n = (S[a |-> t])^(2n + 1)

% Non-reflexive can-rewrite: an equality that can't be composed w/ itself????

For a given ``work item'' \((\alpha \sim \tau)\), we may insert it into the inert set \(S\) if:

\begin{description}
\item[(T1)] \(S^\ast(\alpha) = \alpha\)
\item[(T2)] \(S^\ast(\tau) = \tau\)
\item[(T3)] \(\alpha \notin \fv(\tau)\)
\end{description}

We show that if these conditions are met, an inert set \(S[\alpha \mapsto \tau]\) is still acyclic, and thus has a strong fixed point.

\begin{proof}
  Suppose not, that while \(S\) is acyclic and T1, T2 hold of \((\alpha \sim \tau)\), there is a cycle in \(S[\alpha \mapsto \tau]\). Any such cycle must involve the atomic substitution \(\alpha \mapsto \tau\), as otherwise it would exist in \(S\). This means that there must be a cycle on \(\alpha\). By T3, we know that such a cycle requires some free variable in \(\tau\) to be mapped by \(S^\ast\) to a type with \(\alpha\) as a free variable (this relies on functionality of \(S\)?). Yet by T2, we know that \(S^\ast\) is the identity on \(\tau\), which means that it cannot map any variable in \(\tau\) to \(\alpha\).
\end{proof}

% a -> b, c -> a, b -> c
% [a -> b, c -> b]

% Look at Conor's paper on unification again

\section{A Constraint Solving Algorithm}

We now present a simple algorithm which takes a set of \textsc{given} constraints and a set of \textsc{wanted} constraints, and determines if every \textsc{wanted} constraint is implied by the set of \textsc{given} constraints. We do this in two phases: first, we process the \textsc{given} constraints to form a composite concrete substitution called the ``inert set.'' During this process, we abort the algorithm if we detect obvious inconsistencies in the set of \textsc{given} constraints. If we produce a substitution, we apply it to each \textsc{wanted} in turn, and iff we can rewrite all of them to a reflexive equality (i.e. \(\tau \sim \tau\)), we succeed.

As noted in earlier sections, we would like to show \emph{soundness}, \emph{termination}, and \emph{completeness} of this algorithm.

To write specifically about such an algorithm, we must be precise about what grammar describes our types. Our types take the form
\begin{equation*}
  \tau ::= \text{\sc t} \mid \alpha \mid \tau\ \tau
\end{equation*}
where we write \(\tau, \sigma, \rho\, \pi\) for types, \(\alpha, \beta, \gamma\) for type variables, and {\sc t}, {\sc s} for ground types (constants) like Int, List, etc. We will extend this grammar later to include type-level functions.

Formally, a substitution is a function over \emph{type variables}. However, we can naturally extend it to work over \emph{types} as well.

\begin{equation*}
  S(t) =
  \begin{cases}
    \text{\sc t} & \text{if } t = \text{\sc t}\\
    S(\alpha) & \text{if } t = \alpha\\
    S(\tau)\,S(\sigma) & \text{if } t = \tau\,\sigma
  \end{cases}
\end{equation*}

We also wish to distinguish between mere \emph{equalities} \((\tau \sim \sigma)\) and what we've been calling \emph{atomic rewrites} \([\alpha \mapsto \tau]\). The latter of these can be used to alter substitutions directly.
\begin{equation*}
  S[\alpha \mapsto \tau](\beta) =
  \begin{cases}
    \tau & \text{if } \beta = \alpha\\
    S(\beta) & \text{otherwise}
  \end{cases}
\end{equation*}

The initial sets of \textsc{given} and \textsc{wanted} constraints are equalities between arbitrary types; that is, they are of the form \(\tau \sim \sigma\). We will call the set of yet-to-be-processed \textsc{given} equalities the ``work set'' (deviating from the nomenclature in GHC to emphasize that we will assume nothing about ordering within this structure). Initially, the work set is the set of \textsc{given} equalities. For each step of the solver, we remove an arbitrary \textsc{given} from the work set and process it. This processing may modify the inert set, and may also produce more \textsc{given} equalities to be deferred for later processing.

We split this processing into several parts. First, we use the inert set to rewrite a \text{\sc given} equality, and then extract the ``leftmost'' equality from it, leaving behind a set of other equalities made from the left-behind pieces of its two types. We then attempt to insert this leftmost equality into the inert set, ``kicking out'' any atomic rewrite in the inert set which conflicts with it. We finally add the kicked-out equalities as well as the leftover parts of the original \text{\sc given} equality to the work-list, and continue with the solver's loop.

In the below presentation, we will consider throwing an {\bf error} an ambient effect not reflected in the types shown. Errors are never recovered from; they immediately abort the solving process, as they indicate a contradiction in the inert set.

\begin{equation*}
  \text{\bf canonicalize} : \text{Equality} \to \text{Maybe}~\text{Rewrite} \times \text{Set}~\text{Equality}
\end{equation*}
\begin{equation*}
  \text{\bf canonicalize}(e) =
  \begin{cases}
    \text{\bf Just}~[\alpha \mapsto \tau], \varnothing &
    \begin{minipage}[h]{13em}
      \vspace{.35em}
      \(
      \text{if } e = \alpha \sim \tau\\
      \lor e = \tau \sim \alpha
      \)\\
      \vspace{-.6em}
    \end{minipage}\\
    \text{\bf Nothing}, \varnothing & \text{if } e = \text{\sc t} \sim \text{\sc t}\\
    \error &
    \begin{minipage}[h]{15em}
      \vspace{.35em}
      \(
      \text{if } e = \text{\sc t} \sim \text{\sc s}\ (\text{\sc t} \ne \text{\sc s})\\
            \lor e = \text{\sc t} \sim \tau\, \sigma\\
            \lor e = \tau\, \sigma \sim \text{\sc t}
      \)\\
      \vspace{-.6em}
    \end{minipage}\\
    \begin{minipage}[h]{15em}
      \vspace{.35em}
      \(
      \text{\bf let}~l, R = \text{\bf canonicalize}(\tau \sim \sigma)\\
      \text{\bf in}~~l, R \cup \{\pi \sim \rho\}
      \)\\
      \vspace{-.6em}
    \end{minipage}
    & \text{if } e = \tau\, \pi \sim \sigma\, \rho\\
  \end{cases}
\end{equation*}
Here, we need have a \(\text{\bf choice} : \text{Set}~x \to \text{Maybe}~(x, \text{Set}~x)\). We specify that \(\text{\bf choice}(S) = \text{\bf Nothing} \iff S = \varnothing\) and \(\text{\bf choice}(S) = \text{\bf Just}(x, S^\prime) \implies x \in S \land x \notin S^\prime \land \{x\} \cup S^\prime = S\). In practice, a concrete set may be represented as a list, balanced tree, etc.; choice merely needs to pick \emph{some} element of the set, and we assume nothing about which element it picks. With this operation, we can write the core solver loop:
\begin{equation*}
  \text{\bf step} : \text{Substitution} \times \text{Set}~\text{Equality} \to \text{Substitution} \times \text{Set}~\text{Equality}
\end{equation*}
\begin{align*}
  \text{\bf step}&(\inertset, W) =\\
      &\!\!\!\!\!\text{\bf case}~\text{\bf choice}(W)~\text{\bf of}\\
      &\text{\bf Nothing} \to \inertset, \varnothing\\
      &\text{\bf Just}~(t \sim s, W^\prime) \to\\
      &~~~\text{\bf let}~[\alpha \mapsto \tau], R
      = \text{\bf canonicalize}(\inertset^\ast(t) \sim \inertset^\ast(s))\\
      &~~~\text{\bf in}~
        \begin{cases}
          \inertset[\alpha \mapsto \tau], W^\prime \cup R \cup \{\alpha \sim \inertset(\alpha)\} & \text{if }\alpha \ne \inertset(\alpha) \land \alpha \notin \fv(\tau)\\
          \inertset,\phantom{[\alpha \mapsto \tau]} W^\prime \cup R & \text{if }\alpha = \inertset(\alpha)\land\alpha \notin \fv(\tau)\\
          \text{\bf error} & \phantom{\alpha = \inertset(\alpha)\land\ \,}\text{if }\alpha \in \fv(\tau)
        \end{cases}
\end{align*}

To invoke the solver on some initial set \(G\) of \textsc{given}s, we merely find the fixed point of \(\text{\bf step}(\text{\bf id}, G)\), which will be some substitution \(\inertset\) paired with \(\varnothing\).
\begin{equation*}
  \text{\bf solve} : \text{Set}~\text{Equality} \to \text{Substitution}
\end{equation*}

\section{Soundness}

Soundness of this algorithm can be broken up into two parts:

\begin{itemize}
\item \textbf{step} preserves substitutionality of the inert set
\item the result of \textbf{step} never rewrites two type variables to the same type if the \textsc{given} set's  consistently shows them to have have different types
\end{itemize}

\subsection{Substitutionality}

We previously showed that we can preserve acyclicity by making sure that every augmentation of a substitution \(S[\alpha \mapsto \tau]\) meets three criteria:
\begin{description}
\item[(T1)] \(S^\ast(\alpha) = \alpha\)
\item[(T2)] \(S^\ast(\tau) = \tau\)
\item[(T3)] \(\alpha \notin \fv(\tau)\)
\end{description}
These are all ensured by \textbf{step}. For any substitution \(S\) and types \(s\) and \(t\), we know \(S(\alpha) = \alpha\) and \(S(\tau) = \tau\) for any \([\alpha \mapsto \tau], \_ = \text{\bf canonicalize}(S(t) \sim S(s))\).\footnote{Proof: by induction on the structure of types and the definition of what it means to lift a substitution to apply to types.} Because of this, the atomic rewrite we get out of \(\text{\bf canonicalize}(\inertset^\ast(t) \sim \inertset^\ast(s))\) is on both sides a fixed point of \(\inertset\) (i.e. T1 and T2 hold). And since \textbf{step} performs the ``occurs check'' of T3 itself, this property is also preserved.

Thus, our continued use of \(\inertset^\ast\) as a well-defined substitution is justified.

\subsection{Consistency}



% Because \textbf{leftmost} only generates atomic rewrites which are formed by the application of generativity/injectivity of type constructors over the initial given equalities, it will never hand back an atomic rewrite which disrespects type equality. If the \textsc{given} set is consistent, any transitive closure of such atomic rewrites cannot disrespect any of the provided type equalities.

\section{Termination}

To prove that this \textbf{solve} operation is well-defined, we need to show that such a fixed point always exists. Since the termination condition of the \textbf{solve} operation is when the work set is empty, it suffices to exhibit some bounded-below metric over the pair of the work set and inert set which \textbf{step} causes to decrease with every invocation.

We define our metric \(M : \text{Set}~\text{Equality} \times \text{Substitution} \to \mathbb{N} \times \mathbb{N} \times \mathbb{N}\) as having a three-dimensional total lexicographic ordering. That is, if \(M(W_1, \inertset_1) = l_1, m_1, n_1\) and \(M(W_2, \inertset_2) = l_2, m_2, n_2\),
\begin{alignat*}{1}
  M(W_1, \inertset_1) < M(W_2, I_2) \iff (&l_1 < l_2)\\
                   \lor\hspace{1.05em}(&l_1 = l_2 \land m_1 < m_2)\\
                   \lor\hspace{1.05em}(&l_1 = l_2 \land m_1 = m_2 \land n_1 < n_2)
\end{alignat*}
We define our metric \(M\) as a triple of summations of various characteristics of individual types in the work set.
\begin{align*}
  M(W, \inertset) = &\!\!\!\!\!\sum_{\tau\,\in\,\text{types}(W)}\!\!\!\!\!\!\!\!\text{irreflexive}(\inertset, \tau), \!\!\!\!\!\sum_{\tau\,\in\,\text{types}(W)}\!\!\!\!\!\!\!\!|\tau|,\ |W|
\end{align*}
This definition relies on several other metrics we need to define.

The set of types in the work set is the union of all types in the left- and right-hand sides of every equality.
\begin{align*}
  \text{types}(W) =    &~\{\tau \mid \exists \sigma, \tau \sim \sigma \in W \}\\
                  \cup &~\{\sigma \mid \exists \tau, \tau \sim \sigma \in W \}
\end{align*}

The size of a type is how structurally big it is, determined by number of variables, constants, and applications combined.
\begin{align*}
  |\text{\sc t}| &= 1\\
  |\alpha|       &= 1\\
  |\tau\,\sigma| &= 1 + |\tau| + |\sigma|
\end{align*}

The only nontrivial metric we need to define is \(\text{irreflexive}(\inertset, \tau)\): the number of variable name occurrences in \(\tau\) which are mapped to something other than themselves by \(\inertset\). That is,
\begin{align*}
  \text{irreflexive}(\inertset, \tau) &= \!\!\!\!\sum_{\substack{\alpha\,\in\,\fv(\tau)\\\alpha\,\ne\,\inertset(\alpha)}}\!\!\!\text{count}(\alpha, \tau)
\end{align*}
\begin{alignat*}{3}
  &\text{count}(\alpha, \alpha) &&= 1 &&\\
  &\text{count}(\alpha, \beta)  &&= 0 &&\\
  &\text{count}(\alpha, \text{\sc t}) &&= 0 &&\\
  &\text{count}(\alpha, \tau\,\sigma) &&= \text{count}(\alpha, \tau) + \text{count}(\alpha, \sigma) &&
\end{alignat*}

To show that the metric \(M\) is decreasing for \textbf{step}, we must show that for every \textbf{step} we take, either
\begin{enumerate}
\item The number of irreflexively mapped variable name occurrences in the work set decreases
\item The number of irreflexively mapped variable name occurrences in the work set stays constant, but the cumulative size of all types in the work set decreases
\item The number of irreflexively mapped variable name occurrences in the work set \emph{and} the cumulative size of all types in the work set stay constant, but the size of the work set decreases
\end{enumerate}

% Size only increases when we rewrite with inert set, which decreases irreflexive occurrences
% If no rewriting happens, size can only decrease or stay the same
% If size stays the same (that is, no leftovers), work set cardinality decreases... except when we kick out in these circumstances, it stays the same.

% THE KEY: Kicking out is equivalent to kicking out after rewriting with new inert set! This means that in kicking-out circumstances, we decrease the irreflexive metric, since we trade an (a |-> t) for some (r ~ s) where neither r nor s mention a. The actual algorithm defers this rewriting until the kicked-out item is reprocessed, but these two orderings are equivalent (just, one is easier to reason about).

\subsection{Completeness}

\end{document}